4 2 0 2 c e D 4 1 ] h p - p e h [ 3 7 7 8 6 0 6 / t i m b u s : v i X r a Pretrained Event Classification Model for High Energy Physics Analysis Joshua Ho, Ryan Roberts, Shuo Han, and Haichen Wang Department of Physics, University of California, Berkeley, CA 94720 Physics Division, Lawrence Berkeley National Laboratory, Berkeley CA 94720 (Dated: December 14, 2024) We introduce a foundation model for event classification in high-energy physics, built on a Graph Neural Network architecture and trained on 120 million simulated proton-proton collision events spanning 12 distinct physics processes. The model is pretrained to learn a general and robust representation of collision data using challenging multiclass and multilabel classification tasks. Its performance is evaluated across five event classification tasks, which include both physics processes used during pretraining and new processes not encountered during pretraining. Fine-tuning the pre- trained model significantly improves classification performance, particularly in scenarios with limited training data, demonstrating gains in both accuracy and computational efficiency. To investigate the underlying mechanisms behind these performance improvements, we employ a representational similarity evaluation framework based on Centered Kernel Alignment. This analysis reveals notable differences in the learned representations of fine-tuned pretrained models compared to baseline mod- els trained from scratch. I. INTRODUCTION Machine learning has become a ubiquitous tool in par- ticle physics, employed in a variety of tasks including triggering, simulation, reconstruction, and offline anal- ysis. While its utility spans classification, regression, and generative tasks, the current paradigm of developing machine learning models from scratch for each specific application presents several challenges. This approach not only demands specialized expertise and substantial computing resources but can also result in suboptimal performance due to limited training data. The from- scratch development of models necessitates individual validation studies to ensure that neural networks utilize well-modeled information from training samples, whether derived from Monte Carlo simulations or control samples from experimental data. Foundation models offer a promising direction to ad- dress these limitations. These models, pre-trained on large, diverse datasets across various tasks, provide ro- bust and general representations of underlying data structures. Notable examples in other fields include GPT-4 [1] and BERT [2] in natural language process- ing, Stable Diffusion [3, 4] in image processing, and AlphaFold [5] in structurealbiology. The foundation model approach offers several advantages for particle reduced computing resources for physics applications: fine-tuning [6] compared to training from scratch, supe- rior performance on specific tasks (particularly with lim- ited training data), and potentially simplified validation procedures as downstream tasks inherit verified represen- tations from the pre-trained model. Current literature on pretrained models for particle physics can be categorized based on the data represen- tation they handle. Models operating on particle- or event-level numerical data use features like particle four momenta or jets, leveraging self-supervised or generative methods to learn versatile representations. Detector- focused model operates on high-dimensional responses such as calorimeter deposits or pixel hits, employing geometry-aware techniques for accurate simulation and analysis. Finally, models using textual or code repre- sentations apply large language model architectures to integrate domain knowledge, enabling tasks like question answering and code generation. Recent studies have begun exploring foundation mod- els tailored to particle physics data, which has a vari- ety of distinct structures and properties across many ex- periments and data processing stages, including particle- level & event-level numeric data [7–13], dectector-level & geometry-aware data [14–17], and textual or code data [18]. This paper presents a foundation model designed specifically for collider event-level data. In modern col- lider experiments, final-stage analysis processes informa- tion from reconstructed objects that either directly corre- spond to particles in collision final states (such as leptons and photons) or serve as proxies (such as jets and miss- ing transverse energy). While traditional approaches of- ten relied on “high-level” variables calculated from object features, recent trends favor direct input of event objects and their features into neural networks for analysis tasks. A notable example is Ref. [19], which established the ob- servation of simultaneous production of four top quarks with the ATLAS experiment by employing a graph neural network (GNN) architecture to process event-level object information. We present foundation models that adopt an architec- tural similar to that used for Ref. [19]. Our models are pre-trained using either multiclass classification or multi- label learning tasks across 12 distinct physics processes. We evaluate these models through fine-tuning and testing on five classification tasks, including both familiar and novel processes not seen during pre-training. Our analy- sis benchmarks the models’ performance improvements, their scaling behavior with training sample size, and com- putational efficiency, representing the first prototype of a foundation model operating on collider final-state object data. II. DATA SAMPLES To provide a diverse set of physics processes for the pretraining, we use Madgraph@NLO 2.7.3 [20] to gener- ate proton-proton collision events at next-to-leading or- der (NLO) in Quantum Chromodynamics (QCD). We generate 12 distinct Standard Model (SM) physics pro- cesses, including six major Higgs boson production mech- anisms: gluon fusion production (ggF ), vector boson fusion (V BF ), associated production of the Higgs bo- son with a W boson (W H) or a Z boson (ZH), asso- ciated production of the Higgs boson with a top-quark pair (t¯tH), and associated production of the Higgs bo- son with a single top quark and a forward quark (tHq). Additionally, we simulate six top quark production pro- cesses: single top production, top-quark pair production (t¯t), top quark pair production in association with a pair of photons (t¯tγγ), associated production of a top-quark pair with a W boson (t¯tW ), simultaneous production of three top quarks (t¯tt), and simultaneous production of four top quarks (t¯tt¯t). In these samples, the Higgs boson and top quarks decay inclusively. These 12 Higgs and top quark production processes constitute the pretrain- ing dataset. To test the pretrained model, we further generated four processes including three beyond Standard Model (SM) processes: a SM t¯tH production where the Higgs boson decays exlusiveley to a pair of photons, a t¯tH production with the Higgs boson decaying to a pair of photons, where the top-Yukawa coupling is CP-odd, im- plemented using the Higgs Characterization model [21], the production of a pair of superpartners of the top quark (s-top) using the Minimal Supersymmetric Stan- dard Model (MSSM) [22, 23], and flavor changing neutral current (FCNC) processes [24, 25]. For the s-top process, we simulate the production of heavier s-top pairs (t2 ¯t2), where each heavier s-top (mass 582 GeV) decays into a lighter s-top (t1 or ¯t1, mass 400 GeV) and a Higgs boson. The FCNC process involves t¯t production where one top quark decays to a Higgs boson and a light quark. We generate 10 million events for each process, except for tHq and t¯tt¯t, where 5 million events were produced. In all simulation samples, the center of mass energy of the proton-proton collision is set to 13 TeV. The Higgs boson, top quarks, and vector bosons are set to decay inclusively (except the t¯tH → γγ samples), with MadSpin [26] handling the decays of top quarks and W bosons. The generated events are processed through Pythia 8.235 [27] for parton showering and heavy par- ticle decays, followed by Delphes 3.4.2 [28] configured to emulate the ATLAS detector [29] for fast detector simu- lation. The detector-level object selection criteria are defined to align with typical experimental conditions. Photons are required to have transverse momentum pT ≥ 20 GeV 2 and pseudorapidity |η| ≤ 2.37, excluding the electromag- netic calorimeter crack region (1.37 < |η| < 1.52). Elec- trons must have pT ≥ 10 GeV and |η| ≤ 2.47 (excluding the same crack region), while muons are selected with pT ≥ 10 GeV and |η| ≤ 2.7. Jets are reconstructed using the anti-kt algorithm [30] with radius parameter ∆R = 0.4, where ∆R is defined as (cid:112)∆η2 + ∆ϕ2, with ∆η being the difference in pseudorapidity and ∆ϕ the difference in azimuthal angle. Jets must satisfy pT ≥ 25 GeV and |η| ≤ 2.5. To avoid double-counting, jets are removed if they are within ∆R < 0.4 of a photon or lepton. The identification of jets originating from b-quark decays (b- tagging) is performed by matching jets within ∆R = 0.4 of a b-quark, with efficiency corrections applied to match the performance of the ATLAS experiment’s b-tagging algorithm [31]. III. METHODS A. Overview We present a methodology for developing and eval- uating a foundation model for particle collision event analysis. The approach centers on pretraining a Graph Neural Network (GNN) architecture using a comprehen- sive dataset that spans multiple physics tasks, enabling the model to learn robust and transferable features. For task-specific applications, we employ a fine-tuning strat- egy that combines output layer adaptation with carefully calibrated learning rates for updating the pretrained pa- rameters. Given the prevalence of classification problems in par- ticle physics data analysis, we evaluate the model’s effi- cacy through a systematic assessment across five binary classification tasks: • t¯tH(→ γγ) with CP-even versus CP-odd t-H inter- action • t¯t with FCNC top quark decays versus tHq pro- cesses • t¯tW versus ttt processes • Stop pair production with Higgs bosons in the de- cay chain versus t¯tH processes • W H versus ZH production modes Our evaluation metrics encompass classification per- formance, computational efficiency, and model inter- pretability. The investigation extends to analyzing the model’s scaling behavior with respect to training dataset size, benchmarked against models trained without pre- training. Although we explored transfer learning through parameter freezing of pretrained layers, this approach did not yield performance improvements, leading us to focus our detailed analysis on fine-tuning strategies. This methodological framework demonstrates the po- tential of foundation models to enhance the efficiency of particle physics analyses while improving task-specific performance, offering a promising direction for future high-energy physics research. B. GNN Architecture T We implement a Graph Neural Network (GNN) ar- chitecture that naturally accommodates the point-cloud structure of particle physics data, employing the DGL framework with a PyTorch backend [32][33]. A fully con- nected graph is constructed for each event, with nodes corresponding to reconstructed jets, electrons, muons, photons, and ⃗Emiss . The features of each node include the four-momentum (pT , η, ϕ, E) of the object with a massless assumption (E = pT cosh η), the b-tagging label (for jets), the charge (for leptons), and an integer label- ing the type of object represented by the node. We use a placeholder value of 0 for features which are not defined for every node type such as the b-jet tag, lepton charge, or the pseudorapidity of ⃗Emiss . We assign the angular T distances (∆η, ∆ϕ, ∆R) as edge features and the number of nodes N in the graph as a global feature. We denote the node features {⃗xi}, edge features {⃗yij}, and global features {⃗z}. The GNN model is based on the graph network archi- tecture described in [34] using simple multilayer percep- tron (MLP) feature functions and summation aggrega- tion. The model is comprised of three primary compo- nents: an encoder, the graph network, and a decoder. In the encoder, three MLPs embed the nodes, edges, and global features into a latent space of dimension 64. The graph network block, which is designed to facilitate mes- sage passing between different domains of the graph, per- forms an edge update fe, followed by a node update fn, and finally a global update fg, all defined below. The inputs to each update MLP are concatenated. ⃗y′ ij = fe ({⃗xk}, ⃗yij, ⃗z) = MLP (⃗xi, ⃗xj, ⃗yij, ⃗z) ⃗x′ i = fn (cid:16) ⃗xi, {⃗y′ (cid:17) jk}, ⃗z  = MLP ⃗xi,  ⃗y′ ij, ⃗z  (cid:88) j (cid:16) ⃗z′ = fg {⃗x′ i}, {⃗y′ ij}, ⃗z (cid:17)   = MLP (cid:88)  (cid:88) ⃗x′ i, ⃗y′ ij, ⃗z  i i,j This graph block is iterated four times with the same update MLPs. Finally, the global features are passed through a decoder MLP and a final layer linear to pro- duce the desired model outputs. Each MLP consists of 4 linear layers, each with an output width of 64, with the 3 ReLU activation function. The output of the MLP is then passed through a LayerNorm layer[35]. The total number of trainable parameters in this model is about 400,000. As a performance benchmark, a baseline GNN model is trained from scratch for each classification task. The initial learning rate is set to 10−4 with an exponential decay following LR(x) = LRinitial · (0.99)x, where x rep- resents the epoch number. C. Pretraining Strategy We explore two complementary pretraining approaches to develop robust representations of collision events: (1) multi-class classification, which trains the model to distinguish between different physics processes, and (2) multi-label classification, which predicts the exis- tence and kinematics of heavy particles with prompt de- cays. The pretraining dataset consists of approximately 120 million events, evenly distributed across 12 distinct physics processes, including all major Higgs boson pro- duction mechanisms and top quark processes as described in Section II. This large-scale pretraining effort was con- ducted on the Perlmutter supercomputer at NERSC. 1. Multi-class Classification For Monte Carlo simulated events, the underlying physics process that generated each event is known pre- cisely, providing natural labels for supervised learning. However, the challenge lies in the complexity of collision events: different physics processes can produce similar kinematics and event topologies, particularly in certain regions of phase space. No single observable can unam- biguously identify the underlying process. By training the model to distinguish between 12 different processes simultaneously, we challenge it to learn subtle differences in kinematics and topology that collectively character- ize each process. The model is trained using categorical cross entropy as the loss function. The output layer of the multiclass classification model has 832 trainable pa- rameters. 2. Multi-label Classification This approach combines both classification and regres- sion tasks to characterize collision events. For discrete properties like particle presence in specific kinematic re- gions, we employ classification labels with binary cross- entropy loss. For continuous quantities like particle mul- tiplicities, we use regression labels with mean-squared er- ror loss. This hybrid approach enables the model to learn both categorical and continuous aspects of the physics processes simultaneously. We develop a comprehensive set of 41 labels that cap- ture both particle multiplicities and kinematic proper- ties. This approach increases prediction granularity and enhances model interpretability. By training the model to predict event kinematics rather than event identifica- tion, we create a task-independent framework that can potentially generalize better to novel scenarios not seen during pretraining. The particle multiplicity labels count the number of Higgs bosons (nhiggs), top quarks (ntops), vector bosons (nV ), W bosons (nW ), and Z bosons (nZ). The kine- matic labels characterize the transverse momentum (pT ), pseudorapidity (η), and azimuthal angle (ϕ) of Higgs bosons and top quarks through binned classifications. For Higgs bosons, pT is categorized into three ranges: (0, 30) GeV, (30, 200) GeV, and (200, ∞) GeV, with the upper range particularly sensitive to potential BSM ef- fects. Similarly, both leading and subleading top quarks have pT classifications spanning (0, 30) GeV, (30, 300) GeV, and (300, ∞) GeV. When no particle exists within a specific pT range, the corresponding label is set to [0, 0, 0]. For all particles, η measurements are divided into 4 bins with boundaries at [−1.5, 0, 1.5], while ϕ measurements use 4 bins with boundaries at [− π 2 ]. As with pT , both η and ϕ labels default to [0, 0, 0, 0] in the absence of a particle. This comprehensive labeling schema enables fine-grained learning of kinematic distributions and par- ticle multiplicities, essential for characterizing complex collision events. 2 , 0, π The loss function combines individual losses from all 41 labels through weighted averaging. Binary cross-entropy is applied to classification labels, while mean-squared er- ror is used for regression labels. The model generates predictions for all labels simultaneously, with individ- ual losses calculated according to their respective types. The final loss is computed as an equally-weighted av- erage across all labels, with weights set to 1 to ensure uniform contribution to the optimization process. The output layer of the multilabel model has 2,688 trainable parameters. 3. Pretraining During pre-training, the initial learning rate is 10−4, and the learning rate decays by 1% each epoch following the power law function LR(x) = 10−4 · (0.99)x, where x is the number of epochs. Both pre-trained models reach a plateau in loss by epoch 50, at which point the training is stopped. D. Fine-tuning Methodology For downstream tasks, we adjust the model architec- ture for fine-tuning by replacing the original output layer (final linear layer) with a newly initialized linear layer while retaining the pre-trained weights for all other lay- ers. This modification allows the model to specialize in 4 the specific downstream task while leveraging the general features learned during pretraining. The fine-tuning process begins with distinct learning rate setups for different parts of the model. The newly initialized linear layer is trained with an initial learning rate of 10−4, matching the rate used for models trained from scratch. Meanwhile, the pre-trained layers are fine- tuned more cautiously with a lower initial learning rate of 10−5. This approach ensures that the pre-trained lay- ers adapt gradually without losing their general features, while the new layer learns effectively from scratch. Both learning rates decay over time following the same power law function, LR(x) = LRinitial · (0.99)x, to promote stable convergence as training progresses. We also evaluated a transfer learning setup in which either the decoder MLP or the final linear layer was re- placed with a newly initialized component. During this process, all other model parameters remained frozen, leveraging the pre-trained features without further up- dating them. However, we did not observe performance improvements using the transfer learning setup. Conse- quently, we focus on reporting results obtained with the fine-tuning approach. E. Performance evaluation We assess model performance using two figures of merit: the classification accuracy and the Area Under the Curve (AUC) of the Receiver Operating Characteris- tic (ROC) curve. The accuracy is defined as the fraction of correctly classified events when applying a threshold of 0.5 to the neural network output score. Both metrics demonstrate consistent trends in our analysis. To obtain reliable performance estimates and uncer- tainties, we employ an ensemble training approach where 5 independent models are trained for each configuration with random weight initialization and random subsets of the training dataset. This enables us to evaluate both the models’ sensitivity to initial parameters and to quantify uncertainties in their performance. To investigate how model performance scales with training data, we conducted training runs using sam- ple sizes ranging from 103 to 107 events per class (103, 104, 105, 106, and 107) for each model setup: the from- scratch baseline and models fine-tuned from multi-class or multi-label pretrained models. For the 107 case, only the initialization was randomized due to dataset size lim- itations. All models were evaluated on the same testing dataset, consisting of 2 million events per class, which remained separate from the training process. 5 Name of Task Pretraining Task 103 104 Sample Size 105 106 107 ttH CP Even vs Odd FCNC vs tHq ttW vs ttt stop vs ttH WH vs ZH Baseline Accuracy 56.5 ± 1.1 62.2 ± 0.1 64.3 ± 0.0 65.7 ± 0.0 66.2 ± 0.0 Multiclass (%) +4.8 ± 1.1 +3.4 ± 0.1 +1.3 ± 0.0 +0.2 ± 0.0 -0.0 ± 0.0 Multilabel (%) +2.1 ± 1.2 +1.9 ± 0.1 +0.8 ± 0.1 +0.0 ± 0.0 -0.1 ± 0.0 Baseline Accuracy 63.6 ± 0.7 67.8 ± 0.4 68.4 ± 0.3 69.3 ± 0.3 67.9 ± 0.0 Multiclass (%) +5.8 ± 0.8 +1.2 ± 0.4 +1.4 ± 0.3 +0.5 ± 0.3 -0.0 ± 0.0 -5.3 ± 0.8 -1.3 ± 0.4 +0.9 ± 0.4 +0.3 ± 0.3 +0.4 ± 0.1 Multilabel (%) Baseline Accuracy 75.8 ± 0.1 77.6 ± 0.1 78.9 ± 0.0 79.8 ± 0.0 80.3 ± 0.0 Multiclass (%) +3.7 ± 0.1 +2.7 ± 0.1 +1.3 ± 0.0 +0.4 ± 0.0 +0.0 ± 0.0 Multilabel (%) +2.2 ± 0.1 +1.1 ± 0.1 +0.5 ± 0.0 +0.0 ± 0.0 -0.1 ± 0.0 Baseline Accuracy 83.0 ± 0.2 86.3 ± 0.1 87.6 ± 0.0 88.5 ± 0.0 88.8 ± 0.0 Multiclass (%) +0.4 ± 0.2 +1.9 ± 0.1 +1.0 ± 0.0 +0.3 ± 0.0 +0.0 ± 0.0 Multilabel (%) +2.8 ± 0.2 +1.0 ± 0.1 +0.5 ± 0.0 +0.0 ± 0.0 -0.0 ± 0.0 Baseline Accuracy 51.4 ± 0.1 53.9 ± 0.1 55.8 ± 0.0 57.5 ± 0.0 58.0 ± 0.0 Multiclass (%) +5.2 ± 0.1 +5.3 ± 0.1 +3.1 ± 0.0 +0.6 ± 0.0 +0.1 ± 0.0 -1.1 ± 0.1 -0.9 ± 0.2 +0.5 ± 0.1 +0.1 ± 0.0 -0.1 ± 0.0 Multilabel (%) TABLE I. Accuracy of the traditional model versus the accuracy increase due to fine-tuning from various pretraining tasks. The accuracies are averaged over 5 independently trained models with randomly initialized weights and trained on a random subset of the data. One exception is the 107 training where all models use the same dataset due to limitations on our dataset size. The random subsets are allowed to overlap, but this overlap should be very minimal because all models take an independent random subset of 107 events. The testing accuracy is calculated from the same testing set of 2 million events per class across all models for a specific training task. The errors are the propagated errors (root sum of squares) of the standard deviation of accuracies for each model. IV. RESULTS A. Classification Performance Since the observations of AUC and accuracy show sim- ilar trends, we focus the presentation of the results using accuracy here for conciseness in Table I. In general, the fine-tuned pretrained model achieves at least the same level of classification performance as the baseline model. Notably, there are significant im- provements, particularly when the sample size is small, ranging from 103 to 104 events. In some cases, the accu- racy improvements exceed five percentage points, demon- strating that pretrained models provide a strong initial representation that compensates for limited data. The numerical values of the improvements in accuracy may not fully capture the impact on the sensitivity of the measurements for which the neural network classifier is used, and the final sensitivity improvement is likely to be greater. As the training sample size grows to 105, 106, and even- tually 107 events, the added benefit of pretraining dimin- ishes. With abundant data, models trained from scratch approach or even match the accuracy of fine-tuned pre- trained models. This suggests that large datasets enable effective learning from scratch, rendering the advantage of pretraining negligible in such scenarios. Although both pretraining approaches offer benefits, multiclass pretraining tends to provide more consistent improvements across tasks, especially in the low-data regime. In contrast, multilabel pretraining can some- times lead to neutral or even slightly negative effects for certain tasks and data sizes. This highlights the impor- tance of the pretraining task design, as the similarity be- tween pretraining and fine-tuning tasks in the multiclass approach appears to yield better-aligned representations. Finally, the spread of accuracy across the five tasks for the baseline model is quite large, offering a robust test of fine-tuning across tasks of varying difficulty. The con- sistent observation of these trends across tasks confirms the reliability and robustness of the findings. B. Model Interpretability We aim to understand whether pretrained and base- line models learn the same underlying representations. If the two models exhibit high similarity, a plausible in- terpretation is that pretraining provides the pretrained model with an advantageous initialization, allowing it to converge to a similar state as the baseline model more ef- ficiently. Conversely, significant differences between the models would indicate that pretraining facilitates the de- velopment of a more general and robust latent space, which serves as a foundation for fine-tuning to effectively adapt to the downstream task. To investigate this, we analyzed the representational similarity between a pre- trained model fine-tuned for the downstream task and a baseline model trained directly on the downstream task without pretraining. We use Centered Kernel Alignment (CKA) [36] to an- alyze model similarity and interpretability. CKA is a robust metric that quantifies the similarity between the internal representations of neural networks by comparing their feature matrices in a manner that is invariant to scaling, rotation, and alignment. This invariance makes CKA particularly effective for studying relationships be- tween network layers, even across networks of different sizes or those trained from varying initializations. The similarity is evaluated using a 64-dimensional la- tent representation after the decoder stage of the GNN model. This choice allows us to compare the internal states of the models at a fine-grained level and under- stand how training strategies impact the representations directly used for the output task. To provide an intuitive understanding of CKA values, we construct a table of the CKA scores for various trans- formations performed on a set of dummy data. • A: randomly initialized matrix with shape (1000, 64), following a normal distribution (σ = 1, µ = 0) • B: matrix with shape (1000, 64) constructed via various transformations performed on A • N oise: randomly initialized noise matrix with shape (1000, 64), following a normal distribution (σ = 1, µ = 0) Dataset A, B = A A, B = permutation on columns of A A, B = A + Noise(0.1) A, B = A + Noise(0.5) A, B = A + Noise(0.75) A, B = A · Noise(1) (Linear Transformation) A, B = A + Noise(1) A, B = A + Noise(2) A, B = A + Noise(5) CKA Score 1.00 1.00 0.99 0.80 0.77 0.76 0.69 0.51 0.39 TABLE II. CKA scores for a dummy dataset A and B, where B is created via various transformations performed on A. As seen in Table II and in the definition of the CKA, the CKA score is permutation-invariant. We will use the CKA score to evaluate the similarity between various models and gain insight into the learned representation of detector events in each model (i.e. the information that each model learns). We train ensembles of models for each training task to observe how the CKA score changes due to the random initialization of our models. The CKA score between two models is then defined to be: CKA(A, B) = 1 n2 n (cid:88) n (cid:88) i j CKA(Ai, Bj) (1) where Ai is the representation learned by the ith model in an ensemble with n total models. The error in CKA is the standard deviation of CKA(Ai, Bj). Here we present results for the CKA similarity between the final model in each setup with the final model in the baseline, shown in Table III. The baseline models with different initializations, ex- hibit high similarity values, ranging from approximately 6 Training Task Baseline Multiclass Multilabel ttH CP Even vs Odd 0.94 ± 0.05 0.82 ± 0.01 0.77 ± 0.06 0.96 ± 0.03 0.76 ± 0.01 0.81 ± 0.01 FCNC vs tHq 0.91 ± 0.08 0.75 ± 0.10 0.72 ± 0.05 ttW vs ttt 0.87 ± 0.11 0.79 ± 0.12 0.71 ± 0.08 stop vs ttH 0.90 ± 0.07 0.53 ± 0.03 0.44 ± 0.06 WH vs ZH TABLE III. CKA Similarity of the latent representation be- fore the decoder with the baseline model, averaged over 3 models per training setup, and all models trained with the full dataset (107). The baseline column is not guaranteed to be 1.0 because of the random initialization of the model. Each baseline model converges to a slightly different representation as seen in the CKA values in that column 0.87 to 0.96, which indicates that independently trained baseline models tend to converge on similar internal rep- resentations despite random initialization. Across the considered tasks, models trained as multi-class or multi- label classifiers exhibit noticeably lower CKA similarity scores when compared to the baseline model. For ex- ample, in the WH vs ZH task, the baseline model and another baseline trained model have a high similarity of 0.90, whereas the multi-class and multi-label mod- els show significantly reduced similarities (0.53 and 0.44, respectively). This pattern suggests that the represen- tational spaces developed by multi-class or multi-label models differ substantially from those learned by the baseline model that was trained directly on the down- stream classification task. C. Computational Efficiency To estimate the computational resources required for each approach, we measured the wall time needed for a model to reach its final performance. For baseline mod- els, this is defined as the wall time from the start of training until the loss of the model plateaus. For the foundation model approach, the estimate includes both the pretraining time and the fine-tuning time, each mea- sured from the start of training until the loss plateaus. This approach ensures a consistent and comprehensive evaluation of the computational demands. Fig. 1 shows the fine-tuning time for the model pre- trained with multiclass classification, relative to the time required for the baseline model, as a function of training sample size. In general, the fine-tuning time is signif- icantly shorter than the training time required by the baseline model approach. For smaller training sets, on the order of 105 events, tasks such as FCNC vs. tHq and ttW vs. ttt benefit substantially from the pretrained model’s “head start,” achieving their final performance in only about 1% of the baseline time. For large train- ing datasets, the fine-tuning time relative to the baseline training time becomes larger; however, given that the large training sample typically requires longer training time, fine-tuning still yields much faster training conver- 7 As a practical example, the ATLAS measurement of Higgs boson couplings using the H → γγ decay chan- nel [37] involved training 42 classifiers for event catego- rization. This coincides with our estimate, suggesting that the foundation model approach can reduce compu- tational costs even for a single high-energy physics mea- surement. V. CONCLUSIONS We presented an in-depth study of a particle physics foundation model designed to operate on the four- momentum and identification properties of event final- state objects. This model is built on a Graph Neural Network (GNN) architecture and trained on a dataset comprising 120 million simulated proton-proton collision events across 12 distinct physics processes. The pre- training phase explored both multiclass and multilabel classification tasks, providing a robust foundation for downstream applications. Notably, the pretrained mod- els demonstrated significant improvements in event clas- sification performance when fine-tuned, particularly for tasks with limited training samples. The foundation model approach also offers substantial computational advantages. By leveraging fine-tuning, this methodology reduces the computational resources re- quired for large-scale applications across multiple tasks. Our estimates indicate that significant resource savings can be achieved even for single particle physics measure- ments, making this approach both scalable and efficient. To better understand the learned representations of the pretrained model and guide future optimization efforts, we employed a representational similarity evaluation framework using Centered Kernel Alignment (CKA). This metric allowed us to investigate the source of the performance gains observed in the foundation model. Our analysis revealed notable differences in the learned representations between the fine-tuned pretrained model and a baseline model trained from scratch. In deep learn- ing, it is well-established that multiple equally valid solu- tions can exist. Future studies are necessary to determine whether the low similarity in latent representations re- flects complementary information uniquely captured by the foundation and baseline models, or if it can simply be attributed to connected local minima in the loss land- scape. ACKNOWLEDGMENTS This work is supported by the U.S. National Sci- ence Foundation under the Award No.2046280, and by U.S. Department of Energy, Office of Science under con- tract DE-AC02-05CH11231. Joshua Ho would like to thank the support of UC Berkeley Summer Undergraduate Research Fellowships (SURF) and its donors for this work. FIG. 1. The ratio of the fine-tuning time required to achieve 99% of the baseline model’s final classification accuracy to the total time spent training the baseline model. gence. The ttH CP-even vs. ttH CP-odd task, with a training sample size of 107 events, is an exception where the fine-tuning time exceeds the training time required for the baseline model. This is likely because the pro- cesses involved in this task include photon objects in the final states, which are absent from the events used during pretraining. To accurately evaluate the total time consumption, it is necessary to include the pretraining time required for the foundation model approach. The pretraining times are as follows: • Multi-class pretraining: 45.5 GPU hours • Multi-label pretraining: 60.0 GPU hours The GPU hours recorded for the Multi-label model represent the total time required when training the model in parallel on 16 GPUs. This includes a model synchro- nization step, which results in higher GPU hours com- pared to the Multi-class pretraining model. The foundation model approach becomes increasingly efficient when a large number of tasks are fine-tuned us- ing the same pretrained model, compared to training each task independently from scratch. To illustrate this, we evaluate the computational time required for a scenario where the training sample contains 107 events. For the five tasks tested in this study, the baseline training time (training from scratch) ranges from 1.68 GPU hours (WH vs. ZH) to 5.30 GPU hours (ttW vs. ttt), with an In average baseline training time of 2.94 GPU hours. contrast, the average fine-tuning time for the foundation model approach, relative to the baseline, is 38% of the baseline training time for 107 events. Based on these av- erages, we estimate that the foundation model approach becomes more computationally efficient than the baseline approach when fine-tuning is performed for more than 41 tasks. 105106107Training Sample Size0.00.10.20.30.40.5Fractional TimettH CP Even vs OddFCNC vs tHqttW vs tttstop vs ttHWH vs ZH 8 [1] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, R. Avila, I. Babuschkin, S. Bal- aji, V. Balcom, P. Baltescu, H. Bao, M. Bavarian, J. Belgum, I. Bello, J. Berdine, G. Bernadett-Shapiro, C. Berner, L. Bogdonoff, O. Boiko, M. Boyd, A.-L. Brak- man, G. Brockman, T. Brooks, M. Brundage, K. But- ton, T. Cai, R. Campbell, A. Cann, B. Carey, C. Carl- son, R. Carmichael, B. Chan, C. Chang, F. Chantzis, D. Chen, S. Chen, R. Chen, J. Chen, M. Chen, B. Chess, C. Cho, C. Chu, H. W. Chung, D. Cummings, J. Currier, Y. Dai, C. Decareaux, T. Degry, N. Deutsch, D. Deville, A. Dhar, D. Dohan, S. Dowling, S. Dunning, A. Ecof- fet, A. Eleti, T. Eloundou, D. Farhi, L. Fedus, N. Felix, S. P. Fishman, J. Forte, I. Fulford, L. Gao, E. Georges, C. Gibson, V. Goel, T. Gogineni, G. Goh, R. Gontijo- Lopes, J. Gordon, M. Grafstein, S. Gray, R. Greene, J. Gross, S. S. Gu, Y. Guo, C. Hallacy, J. Han, J. Har- ris, Y. He, M. Heaton, J. Heidecke, C. Hesse, A. Hickey, W. Hickey, P. Hoeschele, B. Houghton, K. Hsu, S. Hu, X. Hu, J. Huizinga, S. Jain, S. Jain, J. Jang, A. Jiang, R. Jiang, H. Jin, D. Jin, S. Jomoto, B. Jonn, H. Jun, T. Kaftan, (cid:32)Lukasz Kaiser, A. Kamali, I. Kanitscheider, N. S. Keskar, T. Khan, L. Kilpatrick, J. W. Kim, C. Kim, Y. Kim, J. H. Kirchner, J. Kiros, M. Knight, D. Koko- tajlo, (cid:32)Lukasz Kondraciuk, A. Kondrich, A. Konstantini- dis, K. Kosic, G. Krueger, V. Kuo, M. Lampe, I. Lan, T. Lee, J. Leike, J. Leung, D. Levy, C. M. Li, R. Lim, M. Lin, S. Lin, M. Litwin, T. Lopez, R. Lowe, P. Lue, A. Makanju, K. Malfacini, S. Manning, T. Markov, Y. Markovski, B. Martin, K. Mayer, A. Mayne, B. Mc- Grew, S. M. McKinney, C. McLeavey, P. McMillan, J. McNeil, D. Medina, A. Mehta, J. Menick, L. Metz, A. Mishchenko, P. Mishkin, V. Monaco, E. Morikawa, D. Mossing, T. Mu, M. Murati, O. Murk, D. M´ely, A. Nair, R. Nakano, R. Nayak, A. Neelakantan, R. Ngo, H. Noh, L. Ouyang, C. O’Keefe, J. Pachocki, A. Paino, J. Palermo, A. Pantuliano, G. Parascandolo, J. Parish, E. Parparita, A. Passos, M. Pavlov, A. Peng, A. Perel- man, F. de Avila Belbute Peres, M. Petrov, H. P. de Oliveira Pinto, Michael, Pokorny, M. Pokrass, V. H. Pong, T. Powell, A. Power, B. Power, E. Proehl, R. Puri, A. Radford, J. Rae, A. Ramesh, C. Raymond, F. Real, K. Rimbach, C. Ross, B. Rotsted, H. Roussez, N. Ry- der, M. Saltarelli, T. Sanders, S. Santurkar, G. Sas- try, H. Schmidt, D. Schnurr, J. Schulman, D. Sel- sam, K. Sheppard, T. Sherbakov, J. Shieh, S. Shoker, P. Shyam, S. Sidor, E. Sigler, M. Simens, J. Sitkin, K. Slama, I. Sohl, B. Sokolowsky, Y. Song, N. Stau- dacher, F. P. Such, N. Summers, I. Sutskever, J. Tang, N. Tezak, M. B. Thompson, P. Tillet, A. Tootoonchian, E. Tseng, P. Tuggle, N. Turley, J. Tworek, J. F. C. Uribe, A. Vallone, A. Vijayvergiya, C. Voss, C. Wain- wright, J. J. Wang, A. Wang, B. Wang, J. Ward, J. Wei, C. Weinmann, A. Welihinda, P. Welinder, J. Weng, L. Weng, M. Wiethoff, D. Willner, C. Winter, S. Wolrich, H. Wong, L. Workman, S. Wu, J. Wu, M. Wu, K. Xiao, T. Xu, S. Yoo, K. Yu, Q. Yuan, W. Zaremba, R. Zellers, C. Zhang, M. Zhang, S. Zhao, T. Zheng, J. Zhuang, W. Zhuk, and B. Zoph, Gpt-4 technical report (2024), arXiv:2303.08774 [cs.CL]. [2] J. Devlin, M. Chang, K. Lee, and K. Toutanova, BERT: pre-training of deep bidirectional transformers for lan- guage understanding, CoRR abs/1810.04805 (2018), 1810.04805. [3] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, High-resolution image synthesis with la- tent diffusion models, CoRR abs/2112.10752 (2021), 2112.10752. [4] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dock- horn, J. M¨uller, J. Penna, and R. Rombach, Sdxl: Im- proving latent diffusion models for high-resolution image synthesis (2023), arXiv:2307.01952 [cs.CV]. [5] J. Jumper, R. Evans, A. Pritzel, et al., Highly accurate protein structure prediction with alphafold, Nature 596, 583 (2021). [6] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, How transferable are features in deep neural networks?, CoRR abs/1411.1792 (2014), 1411.1792. [7] A. J. Wildridge, J. P. Rodgers, E. M. Colbert, Y. yao, A. W. Jung, and M. Liu, Bumblebee: Foundation model for particle physics discovery (2024), arXiv:2412.07867 [hep-ex]. [8] S. Katel, H. Li, Z. Zhao, R. Kansal, F. Mokhtar, and J. Duarte, Learning symmetry-independent jet represen- tations via jet-based joint embedding predictive architec- ture (2024), arXiv:2412.05333 [hep-ph]. [9] T. Golling, L. Heinrich, M. Kagan, S. Klein, M. Leigh, M. Osadchy, and J. A. Raine, Masked particle model- ing on sets: Towards self-supervised high energy physics foundation models (2024), arXiv:2401.13537 [hep-ph]. [10] V. Mikuni and B. Nachman, Omnilearn: A method to simultaneously facilitate all jet physics tasks (2024), arXiv:2404.16091 [hep-ph]. [11] P. Harris, M. Kagan, J. Krupa, B. Maier, and N. Wood- ward, Re-simulation-based self-supervised learning for pre-training foundation models (2024), arXiv:2403.07066 [hep-ph]. [12] J. Birk, A. Hallin, and G. Kasieczka, Omnijet-α: the first cross-task foundation model for particle physics, Machine Learning: Science and Technology 5, 035031 (2024). [13] M. Vigl, N. Hartman, and L. Heinrich, Finetuning foun- dation models for joint analysis optimization (2024), arXiv:2401.13536 [hep-ex]. [14] J. Y. Araz, V. Mikuni, F. Ringer, N. Sato, F. T. Acosta, and R. Whitehill, Point cloud-based diffusion models for the electron-ion collider (2024), arXiv:2410.22421 [hep- ph]. [15] J. Liu, A. Ghosh, D. Smith, P. Baldi, and D. Whiteson, Generalizing to new geometries with geometry-aware au- toregressive models (gaams) for fast calorimeter simula- tion, Journal of Instrumentation 18 (11), P11003. [16] B. Hashemi, N. Hartmann, S. Sharifzadeh, J. Kahn, and T. Kuhr, Ultra-high-granularity detector simulation with intra-event aware generative adversarial network and self- supervised relational reasoning, Nature Communications 15, 10.1038/s41467-024-49104-4 (2024). [17] A. Huang, Y. Melkani, P. Calafiura, A. Lazar, D. T. Mur- nane, M.-T. Pham, and X. Ju, A language model for par- ticle tracking (2024), arXiv:2402.10239 [hep-ph]. [18] Z. Zhang, Y. Zhang, H. Yao, J. Luo, R. Zhao, B. Huang, J. Zhao, Y. Liao, K. Li, L. Zhao, J. Cao, F. Qi, and C. Yuan, Xiwu: A basis flexible and learnable llm for high energy physics (2024), arXiv:2404.08001 [hep-ph]. [19] ATLAS Collaboration (ATLAS), Observation of four- top-quark production in the multilepton final state with the ATLAS detector, Eur. Phys. J. C 83, 496 (2023), [Er- ratum: Eur.Phys.J.C 84, 156 (2024)], arXiv:2303.15061 [hep-ex]. [20] J. Alwall, R. Frederix, S. Frixione, V. Hirschi, F. Maltoni, O. Mattelaer, H. S. Shao, T. Stelzer, P. Torrielli, and M. Zaro, The automated computation of tree-level and next-to-leading order differential cross sections, and their matching to parton shower simulations, JHEP 07, 079, arXiv:1405.0301 [hep-ph]. [21] P. Artoisenet et al., A framework for Higgs characterisa- tion, JHEP 11, 043, arXiv:1306.6464 [hep-ph]. [22] J. Rosiek, Complete set of feynman rules for the minimal supersymmetric extension of the standard model, Phys. Rev. D 41, 3464 (1990). [23] B. Allanach, C. Bal´azs, G. B´elanger, M. Bernhardt, F. Boudjema, D. Choudhury, K. Desch, U. Ell- wanger, P. Gambino, R. Godbole, T. Goto, J. Guasch, M. Guchait, T. Hahn, S. Heinemeyer, C. Hugonie, T. Hurth, S. Kraml, S. Kreiss, J. Lykken, F. Moort- gat, S. Moretti, S. Pe˜naranda, T. Plehn, W. Porod, A. Pukhov, P. Richardson, M. Schumacher, L. Sil- vestrini, P. Skands, P. Slavich, M. Spira, G. Weiglein, and P. Wienemann, Susy les houches accord 2, Computer Physics Communications 180, 8 (2009). [24] C. Degrande, F. Maltoni, J. Wang, and C. Zhang, Au- tomatic computations at next-to-leading order in qcd for top-quark flavor-changing neutral processes, Phys. Rev. D 91, 034024 (2015). [25] G. Durieux, F. Maltoni, and C. Zhang, Global approach to top-quark flavor-changing interactions, Phys. Rev. D 91, 074017 (2015). [26] P. Artoisenet, R. Frederix, O. Mattelaer, and R. Rietkerk, Automatic spin-entangled decays of heavy resonances in Monte Carlo simulations, JHEP 03, 015, arXiv:1212.3460 [hep-ph]. [27] T. Sj¨ostrand, S. Ask, J. R. Christiansen, R. Corke, N. De- sai, P. Ilten, S. Mrenna, S. Prestel, C. O. Rasmussen, and P. Z. Skands, An introduction to PYTHIA 8.2, Comput. Phys. Commun. 191, 159 (2015), arXiv:1410.3012 [hep- ph]. [28] J. de Favereau, C. Delaere, P. Demin, A. Giammanco, V. Lemaˆıtre, A. Mertens, and M. Selvaggi (DELPHES 9 3), DELPHES 3, A modular framework for fast simu- lation of a generic collider experiment, JHEP 02, 057, arXiv:1307.6346 [hep-ex]. [29] T. A. Collaboration, The atlas experiment at the cern large hadron collider, Journal of Instrumentation 3 (08), S08003. [30] M. Cacciari, G. P. Salam, and G. Soyez, The anti-kt jet clustering algorithm, JHEP 04, 063, arXiv:0802.1189 [hep-ph]. [31] G. Aad et al. (ATLAS), ATLAS b-jet identification per- formance and efficiency measurement with t¯t events in s = 13 TeV, Eur. Phys. J. C 79, 970 pp collisions at (2019), arXiv:1907.05120 [hep-ex]. √ [32] M. Wang, L. Yu, D. Zheng, Q. Gan, Y. Gai, Z. Ye, M. Li, J. Zhou, Q. Huang, C. Ma, Z. Huang, Q. Guo, H. Zhang, H. Lin, J. Zhao, J. Li, A. J. Smola, and Z. Zhang, Deep graph library: Towards efficient and scal- able deep learning on graphs, CoRR abs/1909.01315 (2019), 1909.01315. [33] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Brad- bury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K¨opf, E. Z. Yang, Z. De- Vito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, Pytorch: An impera- tive style, high-performance deep learning library, CoRR abs/1912.01703 (2019), 1912.01703. [34] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez- Gonzalez, V. F. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, C¸ . G¨ul¸cehre, H. F. Song, A. J. Ballard, J. Gilmer, G. E. Dahl, A. Vaswani, K. R. Allen, C. Nash, V. Langston, C. Dyer, N. Heess, D. Wierstra, P. Kohli, M. M. Botvinick, O. Vinyals, Y. Li, and R. Pascanu, Relational inductive biases, deep learning, and graph networks, CoRR abs/1806.01261 (2018), 1806.01261. [35] J. L. Ba, J. R. Kiros, and G. E. Hinton, Layer normal- ization (2016), arXiv:1607.06450 [stat.ML]. [36] S. Kornblith, M. Norouzi, H. Lee, and G. E. Hinton, Sim- ilarity of neural network representations revisited, CoRR abs/1905.00414 (2019), 1905.00414. [37] ATLAS Collaboration, Measurement of the properties of s = 13 TeV in the H → γγ Higgs boson production at channel using 139 fb−1 of pp collision data with the AT- LAS experiment, JHEP 07, 088, arXiv:2207.00348 [hep- ex]. √